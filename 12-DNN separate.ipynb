{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You can find the C code in this temporary file: C:\\Users\\Yao\\AppData\\Local\\Temp\\theano_compilation_error__39763pl\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'theano' has no attribute 'gof'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\lazylinker_c.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                     actual_version, force_compile, _need_reload))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\lazylinker_c.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m                         \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                         actual_version, force_compile, _need_reload))\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lazylinker will not be imported if theano.config.cxx is not set.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlazylinker_c\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\lazylinker_c.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m             cmodule.GCC_compiler.compile_str(dirname, code, location=loc,\n\u001b[1;32m--> 140\u001b[1;33m                                              preargs=args)\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[1;31m# Save version into the __init__.py file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\cmodule.py\u001b[0m in \u001b[0;36mcompile_str\u001b[1;34m(module_name, src_code, location, include_dirs, lib_dirs, libs, preargs, py_module, hide_symbols)\u001b[0m\n\u001b[0;32m   2387\u001b[0m             raise Exception('Compilation failed (return status=%s): %s' %\n\u001b[1;32m-> 2388\u001b[1;33m                             (status, compile_stderr.replace('\\n', '. ')))\n\u001b[0m\u001b[0;32m   2389\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompilation_warning\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcompile_stderr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Compilation failed (return status=1): C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `_import_array':\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1470: undefined reference to `__imp_PyExc_ImportError'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1476: undefined reference to `__imp_PyExc_AttributeError'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1481: undefined reference to `__imp_PyCapsule_Type'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1482: undefined reference to `__imp_PyExc_RuntimeError'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1497: undefined reference to `__imp_PyExc_RuntimeError'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1505: undefined reference to `__imp_PyExc_RuntimeError'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1511: undefined reference to `__imp_PyExc_RuntimeError'\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1521: undefined reference to `__imp_PyExc_RuntimeError'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o:C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/__multiarray_api.h:1533: more undefined references to `__imp_PyExc_RuntimeError' follow\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `NpyCapsule_Check':\r. C:/ProgramData/Anaconda3/lib/site-packages/numpy/core/include/numpy/npy_3kcompat.h:456: undefined reference to `__imp_PyCapsule_Type'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `unpack_list_of_ssize_t':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:48: undefined reference to `__imp_PyExc_TypeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:58: undefined reference to `__imp_PyExc_IndexError'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `CLazyLinker_init':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:352: undefined reference to `__imp_PyExc_IndexError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:370: undefined reference to `__imp_PyExc_IndexError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:385: undefined reference to `__imp_PyExc_IndexError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:389: undefined reference to `__imp_PyExc_IndexError'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o:C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:393: more undefined references to `__imp_PyExc_IndexError' follow\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `CLazyLinker_init':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:405: undefined reference to `__imp_PyExc_TypeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:420: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:426: undefined reference to `__imp_PyExc_IndexError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:440: undefined reference to `__imp_PyExc_TypeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:444: undefined reference to `__imp__Py_NoneStruct'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `c_call':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:545: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:545: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:545: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:546: undefined reference to `__imp__Py_NoneStruct'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o:C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:546: more undefined references to `__imp__Py_NoneStruct' follow\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `lazy_rec_eval':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:618: undefined reference to `__imp_PyExc_IndexError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:641: undefined reference to `__imp_PyExc_TypeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:649: undefined reference to `__imp_PyExc_ValueError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:657: undefined reference to `__imp_PyExc_IndexError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:708: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:715: undefined reference to `__imp_PyExc_TypeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:721: undefined reference to `__imp_PyExc_TypeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:771: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:771: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:772: undefined reference to `__imp__Py_NoneStruct'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `CLazyLinker_call':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:814: undefined reference to `__imp_PyExc_RuntimeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:826: undefined reference to `__imp_PyExc_RuntimeError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:839: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:839: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:840: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:849: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:849: undefined reference to `__imp__Py_NoneStruct'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o:C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:850: more undefined references to `__imp__Py_NoneStruct' follow\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `CLazyLinker_call':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:894: undefined reference to `__imp_PyExc_AssertionError'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:937: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:937: undefined reference to `__imp__Py_NoneStruct'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:938: undefined reference to `__imp__Py_NoneStruct'\r. C:\\Users\\Yao\\AppData\\Local\\Temp\\ccbEpN0f.o: In function `CLazyLinker_set_allow_gc':\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:973: undefined reference to `__imp_PyBool_Type'\r. C:/Users/Yao/AppData/Local/Theano/compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_58_Stepping_9_GenuineIntel-3.6.4-64/lazylinker_ext/mod.cpp:976: undefined reference to `__imp__Py_TrueStruct'\r. collect2.exe: error: ld returned 1 exit status\r. ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f0d5ccb5de0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m     object2, utils)\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m from theano.compile import (\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[0mSymbolicInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mSymbolicOutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOut\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\compile\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_module\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\compile\\mode.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\theano\\gof\\vm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    684\u001b[0m     \u001b[1;31m# OSError happens when g++ is not installed.  In that case, we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[1;31m# already changed the default linker to something else then CVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'theano' has no attribute 'gof'"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import mir_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wav\n",
    "#from os import walk\n",
    "import pickle\n",
    "import copy\n",
    "import timeit\n",
    "import inspect\n",
    "import sys\n",
    "import numpy\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "sys.setrecursionlimit(15000)\n",
    "\n",
    "%matplotlib inline\n",
    "try:\n",
    "    sys.getwindowsversion()\n",
    "except AttributeError:\n",
    "    isWindows = False\n",
    "else:\n",
    "    isWindows = True\n",
    "\n",
    "if isWindows:\n",
    "    import win32api,win32process,win32con\n",
    "    pid = win32api.GetCurrentProcessId()\n",
    "    handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, True, pid)\n",
    "    win32process.SetPriorityClass(handle, win32process.HIGH_PRIORITY_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tdataset(point):#load data\n",
    "    for path, dirs, files in os.walk('Wavfile'):\n",
    "        for name in files:\n",
    "            y, sr = librosa.load('Wavfile/'+ name, sr=16000, mono=False)\n",
    "            D = librosa.stft(y[0] + y[1], n_fft = 1024, hop_length = 512)[:,:point]\n",
    "            D1 = librosa.stft(y[0], n_fft = 1024, hop_length = 512)[:,:point]\n",
    "            D2 = librosa.stft(y[1], n_fft = 1024, hop_length = 512)[:,:point]\n",
    "            if D.shape[1] < point:\n",
    "                tmp = np.zeros((513,point - D.shape[1]),dtype = float)\n",
    "                D = np.column_stack((D, tmp))\n",
    "            if D1.shape[1] < point:\n",
    "                tmp = np.zeros((513,point - D1.shape[1]),dtype = float)\n",
    "                D1 = np.column_stack((D1, tmp))\n",
    "            if D2.shape[1] < point:\n",
    "                tmp = np.zeros((513,point - D2.shape[1]),dtype = float)\n",
    "                D2 = np.column_stack((D2, tmp))\n",
    "            magnitude, phase = librosa.magphase(D)\n",
    "            magnitude = magnitude.T.flatten()\n",
    "            phase = phase.T.flatten()\n",
    "            magnitude1, phase1 = librosa.magphase(D1)\n",
    "            magnitude1 = magnitude1.T.flatten()\n",
    "            phase1 = phase1.T.flatten()\n",
    "            magnitude2, phase2 = librosa.magphase(D2)\n",
    "            magnitude2 = magnitude2.T.flatten()\n",
    "            phase2 = phase2.T.flatten()\n",
    "            if name[:7] == 'Aaliyah':\n",
    "                if name == 'Aaliyah_Sample.wav':              \n",
    "                    try:\n",
    "                        dva = np.vstack((dva,magnitude))\n",
    "                        pva = np.vstack((ptr,phase))\n",
    "                    except UnboundLocalError:\n",
    "                        dva = magnitude\n",
    "                        pva = phase\n",
    "                    try:\n",
    "                        d1va = np.vstack((d1va,magnitude1))\n",
    "                        p1va = np.vstack((p1va,phase1))\n",
    "                    except UnboundLocalError:\n",
    "                        d1va = magnitude1\n",
    "                        p1va = phase1\n",
    "                    try:\n",
    "                        d2va = np.vstack((d2va,magnitude2))\n",
    "                        p2va = np.vstack((p2va,phase2))\n",
    "                    except UnboundLocalError:\n",
    "                        d2va = magnitude2\n",
    "                        p2va = phase2\n",
    "                else:\n",
    "                    try:\n",
    "                        dtr = np.vstack((dtr,magnitude))\n",
    "                        ptr = np.vstack((ptr,phase))\n",
    "                    except UnboundLocalError:\n",
    "                        dtr = magnitude\n",
    "                        ptr = phase\n",
    "                    try:\n",
    "                        d1tr = np.vstack((d1tr,magnitude1))\n",
    "                        p1tr = np.vstack((p1tr,phase1))\n",
    "                    except UnboundLocalError:\n",
    "                        d1tr = magnitude1\n",
    "                        p1tr = phase1\n",
    "                    try:\n",
    "                        d2tr = np.vstack((d2tr,magnitude2))\n",
    "                        p2tr = np.vstack((p2tr,phase2))\n",
    "                    except UnboundLocalError:\n",
    "                        d2tr = magnitude2\n",
    "                        p2tr = phase2\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    dte = np.vstack((dte,magnitude))\n",
    "                    pte = np.vstack((pte,phase))\n",
    "                except UnboundLocalError:\n",
    "                    dte = magnitude\n",
    "                    pte = phase\n",
    "                try:\n",
    "                    d1te = np.vstack((d1te,magnitude1))\n",
    "                    p1te = np.vstack((p1te,phase1))\n",
    "                except UnboundLocalError:\n",
    "                    d1te = magnitude1\n",
    "                    p1te = phase1\n",
    "                try:\n",
    "                    d2te = np.vstack((d2te,magnitude2))\n",
    "                    p2te = np.vstack((p2te,phase2))\n",
    "                except UnboundLocalError:\n",
    "                    d2te = magnitude2\n",
    "                    p2te = phase2\n",
    "    trdata = [dtr,d1tr,d2tr]\n",
    "    tedata = [dte,d1te,d2te]\n",
    "    trphase = [ptr, p1tr,p2tr]\n",
    "    tephase = [pte, p1te,p2te]\n",
    "    vadata = [dva,d1va,d2va]\n",
    "    vaphase = [pva,p1va,p2va]\n",
    "    return trdata ,tedata, trphase, tephase, vadata, vaphase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trdata, tedata, trphase, tephase, vadata, vaphase = tdataset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save data if you want\n",
    "'''\n",
    "pickle.dump( trdata, open( \"trdata.p\", \"wb\" ) )\n",
    "pickle.dump( tedata, open( \"tedata.p\", \"wb\" ) )\n",
    "pickle.dump( trphase, open( \"trphase.p\", \"wb\" ) )\n",
    "pickle.dump( tephase, open( \"tephase.p\", \"wb\" ) )\n",
    "pickle.dump( vadata, open( \"vadata.p\", \"wb\" ) )\n",
    "pickle.dump( vaphase, open( \"vaphase.p\", \"wb\" ) )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSLU3(object):\n",
    "    \"\"\" Elman Neural Net Model Class\n",
    "    \"\"\"\n",
    "    def __init__(self, input, nh):\n",
    "        \"\"\"Initialize the parameters for the RNNSLU\n",
    "\n",
    "        :type nh: int\n",
    "        :param nh: dimension of the first hidden layer\n",
    "\n",
    "        :type cs: int\n",
    "        :param cs: input lenth\n",
    "\n",
    "        \"\"\"\n",
    "        # parameters of the model\n",
    "        self.h1 = theano.shared(name='h1', value=numpy.zeros(nh,dtype=theano.config.floatX))\n",
    "        self.h2 = theano.shared(name='h2', value=numpy.zeros(nh,dtype=theano.config.floatX))\n",
    "        self.h3 = theano.shared(name='h3', value=numpy.zeros(nh,dtype=theano.config.floatX))\n",
    "\n",
    "        self.wx1 = theano.shared(name='wx1', value=0.2 * numpy.random.uniform(-1.0, 1.0, (513, nh)).astype(theano.config.floatX))\n",
    "        self.wx2 = theano.shared(name='wx2', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "        self.wx3 = theano.shared(name='wx3', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "\n",
    "        self.wh1 = theano.shared(name='wh1', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "        self.wh2 = theano.shared(name='wh2', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "        self.wh3 = theano.shared(name='wh3', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "\n",
    "\n",
    "        # bundle\n",
    "        self.params = [self.h1, self.h2, self.h3, self.wx1, self.wx2, self.wx3, self.wh1, self.wh2, self.wh3]\n",
    "\n",
    "        # as many columns as context window size\n",
    "        # as many lines as words in the sequence\n",
    "        x = (input.reshape((100,513)))\n",
    "\n",
    "\n",
    "        def recurrence(x_t, h1_tm1,h2_tm1,h3_tm1):\n",
    "            h1_t = T.nnet.relu(T.dot(x_t, self.wx1) + T.dot(h1_tm1, self.wh1))\n",
    "            h2_t = T.nnet.relu(T.dot(h1_t,self.wx2) + T.dot(h2_tm1, self.wh2))\n",
    "            h3_t = T.nnet.relu(T.dot(h2_t,self.wx3) + T.dot(h3_tm1, self.wh3))\n",
    "            return [h1_t, h2_t, h3_t]\n",
    "\n",
    "        [h1,h2,h3], _ = theano.scan(fn=recurrence,\n",
    "                               sequences=x,\n",
    "                               outputs_info=[self.h1, self.h2, self.h3],\n",
    "                               n_steps=x.shape[0])\n",
    "        self.output = h3\n",
    "\n",
    "\n",
    "def shared_dataset(x):\n",
    "    shared_x = theano.shared(numpy.asarray(x,\n",
    "                                           dtype=theano.config.floatX),\n",
    "                             borrow=True)\n",
    "    return shared_x\n",
    "def test_DRNN(trdata ,tedata, vadata, learning_rate=0.1, n_epochs=20, batch_size=1, point = 100):\n",
    "    \"\"\" Demonstrates lenet on MNIST dataset\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the dataset used for training /testing (MNIST here)\n",
    "\n",
    "    :type nkerns: list of ints\n",
    "    :param nkerns: number of kernels on each layer\n",
    "    \"\"\"\n",
    "\n",
    "    rng = numpy.random.RandomState(23455)\n",
    "\n",
    "    train_set_x = shared_dataset(trdata[0])\n",
    "    test_set_x = shared_dataset(tedata[0])\n",
    "    train_set_z = shared_dataset(trdata[0])\n",
    "    test_set_z = shared_dataset(tedata[0])\n",
    "    train_set_y1 = shared_dataset(trdata[1])\n",
    "    test_set_y1 = shared_dataset(tedata[1])\n",
    "    train_set_y2 = shared_dataset(trdata[2])\n",
    "    test_set_y2 = shared_dataset(tedata[2])\n",
    "    valid_set_x = shared_dataset(vadata[0])\n",
    "    valid_set_z = shared_dataset(vadata[0])\n",
    "    valid_set_y1 = shared_dataset(vadata[1])\n",
    "    valid_set_y2 = shared_dataset(vadata[2])\n",
    "\n",
    "    n_train_batches = len(trdata[0])\n",
    "    n_test_batches = len(tedata[0])\n",
    "    n_valid_batches = len(vadata[0])\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "#    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "#    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "#    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "#    n_train_batches //= batch_size\n",
    "#    n_valid_batches //= batch_size\n",
    "#    n_test_batches //= batch_size\n",
    "\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # start-snippet-1\n",
    "    x = T.vector('x')   # the data is presented as rasterized images\n",
    "    y1 = T.vector('y1')  # the labels are presented as 1D vector of\n",
    "    y2 = T.vector('y2')\n",
    "    z = T.vector('z')                    # [int] labels\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # Reshape matrix of rasterized images of shape (batch_size, 32 * 32)\n",
    "    # to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "    # (32, 32) is the size of CIFAR-10 images.\n",
    "    layer0_input = x\n",
    "\n",
    "    # Construct the first convolutional pooling layer:\n",
    "    # filtering reduces the image size to (32-3+1 , 32-3+1) = (30, 30)\n",
    "    # maxpooling reduces this further to (30/2, 30/2) = (15, 15)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 15, 15)\n",
    "    # filter_shape: (number of filters, num input feature maps, filter height, filter width)\n",
    "    layer0 = RNNSLU3(\n",
    "        input=layer0_input,\n",
    "        nh = 150\n",
    "    )\n",
    "    w = theano.shared(name='w', value=numpy.random.uniform(-1.0, 1.0, (150, 1026)).astype(theano.config.floatX))\n",
    "    b = theano.shared(name='b', value=numpy.zeros((1026,), dtype=theano.config.floatX))\n",
    "\n",
    "\n",
    "\n",
    "    tmp = T.nnet.relu(T.dot(layer0.output, w) + b)\n",
    "\n",
    "    tmp1 = tmp[:,:513]\n",
    "    tmp2 = tmp[:,513:]\n",
    "    #out1 = abs(tmp1)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    #out2 = abs(tmp2)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    #cost = T.mean((out1-y1.reshape((point,513)))**2+(out2-y2.reshape((point,513)))**2)\n",
    "    out1 = abs(tmp1)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    out2 = abs(tmp2)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    cost = T.mean((out1-y1.reshape((point,513)))**2+(out2-y2.reshape((point,513)))**2)\n",
    "    #(sdr1, sir1, sar1,perm1) = mir_eval.separation.bss_eval_sources(out1.flatten(),z)\n",
    "    #(sdr2, sir2, sar2,perm2) = mir_eval.separation.bss_eval_sources(y1.flatten(),z)\n",
    "    #error =  sdr1-sdr2\n",
    "    error = cost\n",
    "    # create a function to compute the mistakes that are made by the model\n",
    "\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        error,\n",
    "        givens={\n",
    "            x: test_set_x[index],\n",
    "            y1: test_set_y1[index],\n",
    "            y2: test_set_y2[index],\n",
    "            z: test_set_z[index]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        error,\n",
    "        givens={\n",
    "            x: valid_set_x[index],\n",
    "            y1: valid_set_y1[index],\n",
    "            y2: valid_set_y2[index],\n",
    "            z: valid_set_z[index]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = [w, b] + layer0.params\n",
    "    '''\n",
    "    # create a list of gradients for all model parameters\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # train_model is a function that updates the model parameters by\n",
    "    # SGD Since this model has many parameters, it would be tedious to\n",
    "    # manually create an update rule for each model parameter. We thus\n",
    "    # create the updates list by automatically looping over all\n",
    "    # (params[i], grads[i]) pairs.\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "    '''\n",
    "    updates = Adam(cost, params, lr=0.0002, b1=0.1, b2=0.001, e=1e-8)\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates = updates,\n",
    "        givens={\n",
    "            x: train_set_x[index],\n",
    "            y1: train_set_y1[index],\n",
    "            y2: train_set_y2[index],\n",
    "            z: train_set_z[index]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # end-snippet-1\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 50  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        test_model(i)\n",
    "                        for i in range(n_test_batches)\n",
    "                    ]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f ') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    out_model = theano.function(\n",
    "        [index],\n",
    "        [out1,out2],\n",
    "        givens={\n",
    "            x: valid_set_x[index],\n",
    "            z: valid_set_z[index]\n",
    "        }\n",
    "    )\n",
    "    out_wav=[]\n",
    "    for i in range(n_valid_batches):\n",
    "        out_wav.append(out_model(i))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f  obtained at iteration %i, '\n",
    "          'with test performance %f ' %\n",
    "          (best_validation_loss, best_iter + 1, test_score ))\n",
    "    print(('The code for file ' +\n",
    "           os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n",
    "    return out_wav\n",
    "\n",
    "def Adam(cost, params, lr=0.0002, b1=0.1, b2=0.001, e=1e-8):\n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    i = theano.shared(numpy.float32(0))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - (1. - b1)**i_t\n",
    "    fix2 = 1. - (1. - b2)**i_t\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    updates.append((i, i_t))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train drnn!\n",
    "out=test_DRNN(trdata, tedata, vadata, learning_rate=0.1, n_epochs=1, batch_size=1, point = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transfer data back to wav\n",
    "def istft_mp(mag, phase):\n",
    "    y_hat = librosa.istft(mag*phase, hop_length = 512)\n",
    "    return y_hat\n",
    "\n",
    "# for each song: return sdr, sir, sar for a single song\n",
    "def evaluate(ori_sing, out_sing, mix):\n",
    "    sdr, sir, sar, perm = mir_eval.separation.bss_eval_sources( ori_sing, out_sing )\n",
    "    sdr1, sir1, sar1, perm1 = mir_eval.separation.bss_eval_sources( ori_sing, mix )\n",
    "    nsdr = sdr - sdr1\n",
    "    return nsdr, sir, sar\n",
    "\n",
    "# for all song's evaluation vectors: weighted index\n",
    "'''\n",
    "def g_eval(v_nsdr, v_sir, v_sar, v_length):\n",
    "    sum_length = sum(v_lengeth)\n",
    "    w_length = v_length/sum_length\n",
    "    gnsdr = T.dot(v_nsdr, w_length)\n",
    "    gsir = T.dot(v_sir, w_length)\n",
    "    gsar = T.dot(v_sar, w_length)\n",
    "    return gnsdr, gsir, gsar\n",
    "'''\n",
    "def g_eval(v_nsdr, v_sir, v_sar):\n",
    "    gnsdr = np.mean(v_nsdr)\n",
    "    gsir = np.mean(v_sir)\n",
    "    gsar = np.mean(v_sar)\n",
    "    return gnsdr, gsir, gsar\n",
    "\n",
    "# output wav files\n",
    "def prod_wav(out_sing,mix, ori_right, i):\n",
    "    import os\n",
    "    librosa.output.write_wav(os.path.join(os.getcwd() + '/out/' + '%i'%i + '_gt_mix.wav'),sr=16000,y=mix)\n",
    "    librosa.output.write_wav(os.path.join(os.getcwd() + '/out/' + '%i'%i + '_pred_right.wav'),sr=16000,y=out_sing)\n",
    "    librosa.output.write_wav(os.path.join(os.getcwd() + '/out/' + '%i'%i + '_ori_right.wav'),sr=16000,y=ori_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outphase=tephase[0][0].reshape(100,513).T\n",
    "lphase=tephase[1][0].reshape(100,513).T\n",
    "rphase=tephase[2][0].reshape(100,513).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_nsdr=[]\n",
    "n_sir=[]\n",
    "n_sar=[]\n",
    "for i in range(len(out)):\n",
    "    mag=out[i][0]+out[i][1]\n",
    "    y_hat=istft_mp(mag.T,outphase)\n",
    "    y_left=istft_mp(tedata[1][i].reshape(100,513).T,lphase)\n",
    "    y_right=istft_mp(tedata[2][i].reshape(100,513).T,rphase)\n",
    "    nsdr, sir, sar=evaluate(tedata[0][i],out[i][0].flatten(),tedata[2][i])\n",
    "    n_nsdr.append(nsdr)\n",
    "    n_sir.append(sir)\n",
    "    n_sar.append(sar)\n",
    "    prod_wav(istft_mp(out[i][1].T,rphase),y_hat,y_right,i)\n",
    "gnsdr, gsir, gsar=g_eval(n_nsdr, n_sir, n_sar)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnsdr, gsir, gsar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(5,1,1)\n",
    "plt.plot(tedata[0][0].reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('mixture')\n",
    "plt.subplot(5,1,2)\n",
    "plt.plot(out[0][0].flatten().reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('recovered music')\n",
    "plt.subplot(5,1,3)\n",
    "plt.plot(tedata[1][0].reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('clean music')\n",
    "plt.subplot(5,1,4)\n",
    "plt.plot(out[0][1].flatten().reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('recovered vocal')\n",
    "plt.subplot(5,1,5)\n",
    "plt.plot(tedata[2][0].reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('clean vocal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNSLU3(object):\n",
    "    \"\"\" Elman Neural Net Model Class\n",
    "    \"\"\"\n",
    "    def __init__(self, input, nh):\n",
    "        \"\"\"Initialize the parameters for the RNNSLU\n",
    "\n",
    "        :type nh: int\n",
    "        :param nh: dimension of the first hidden layer\n",
    "\n",
    "        :type cs: int\n",
    "        :param cs: input lenth\n",
    "\n",
    "        \"\"\"\n",
    "        # parameters of the model\n",
    "        self.h1 = theano.shared(name='h1', value=numpy.zeros(nh,dtype=theano.config.floatX))\n",
    "        self.h2 = theano.shared(name='h2', value=numpy.zeros(nh,dtype=theano.config.floatX))\n",
    "        self.h3 = theano.shared(name='h3', value=numpy.zeros(nh,dtype=theano.config.floatX))\n",
    "\n",
    "        self.wx1 = theano.shared(name='wx1', value=0.2 * numpy.random.uniform(-1.0, 1.0, (513, nh)).astype(theano.config.floatX))\n",
    "        self.wx2 = theano.shared(name='wx2', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "        self.wx3 = theano.shared(name='wx3', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "\n",
    "        self.wh1 = theano.shared(name='wh1', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "        self.wh2 = theano.shared(name='wh2', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "        self.wh3 = theano.shared(name='wh3', value=0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
    "\n",
    "\n",
    "        # bundle\n",
    "        self.params = [self.h1, self.h2, self.h3, self.wx1, self.wx2, self.wx3, self.wh1, self.wh2, self.wh3]\n",
    "\n",
    "        # as many columns as context window size\n",
    "        # as many lines as words in the sequence\n",
    "        x = (input.reshape((100,513)))\n",
    "\n",
    "\n",
    "        def recurrence(x_t, h1_tm1,h2_tm1,h3_tm1):\n",
    "            h1_t = T.nnet.relu(T.dot(x_t, self.wx1) + T.dot(h1_tm1, self.wh1))\n",
    "            h2_t = T.nnet.relu(T.dot(h1_t,self.wx2) + T.dot(h2_tm1, self.wh2))\n",
    "            h3_t = T.nnet.relu(T.dot(h2_t,self.wx3) + T.dot(h3_tm1, self.wh3))\n",
    "            return [h1_t, h2_t, h3_t]\n",
    "\n",
    "        [h1,h2,h3], _ = theano.scan(fn=recurrence,\n",
    "                               sequences=x,\n",
    "                               outputs_info=[self.h1, self.h2, self.h3],\n",
    "                               n_steps=x.shape[0])\n",
    "        self.output = h3\n",
    "\n",
    "def IS(x,y):\n",
    "    return T.sum((-T.log(abs(x)/(abs(y)+10**(-8)))+(abs(x)/(abs(y)+10**(-8)))-1))\n",
    "\n",
    "def shared_dataset(x):\n",
    "    shared_x = theano.shared(numpy.asarray(x,\n",
    "                                           dtype=theano.config.floatX),\n",
    "                             borrow=True)\n",
    "    return shared_x\n",
    "def test_DRNN(trdata ,tedata, vadata, learning_rate=0.1, n_epochs=20, batch_size=1, point = 100, isd = False):\n",
    "    \"\"\" Demonstrates lenet on MNIST dataset\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the dataset used for training /testing (MNIST here)\n",
    "\n",
    "    :type nkerns: list of ints\n",
    "    :param nkerns: number of kernels on each layer\n",
    "    \"\"\"\n",
    "\n",
    "    rng = numpy.random.RandomState(23455)\n",
    "\n",
    "    train_set_x = shared_dataset(trdata[0])\n",
    "    test_set_x = shared_dataset(tedata[0])\n",
    "    train_set_z = shared_dataset(trdata[0])\n",
    "    test_set_z = shared_dataset(tedata[0])\n",
    "    train_set_y1 = shared_dataset(trdata[1])\n",
    "    test_set_y1 = shared_dataset(tedata[1])\n",
    "    train_set_y2 = shared_dataset(trdata[2])\n",
    "    test_set_y2 = shared_dataset(tedata[2])\n",
    "    valid_set_x = shared_dataset(vadata[0])\n",
    "    valid_set_z = shared_dataset(vadata[0])\n",
    "    valid_set_y1 = shared_dataset(vadata[1])\n",
    "    valid_set_y2 = shared_dataset(vadata[2])\n",
    "\n",
    "    n_train_batches = len(trdata[0])\n",
    "    n_test_batches = len(tedata[0])\n",
    "    n_valid_batches = len(vadata[0])\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "#    n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "#    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "#    n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "#    n_train_batches //= batch_size\n",
    "#    n_valid_batches //= batch_size\n",
    "#    n_test_batches //= batch_size\n",
    "\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # start-snippet-1\n",
    "    x = T.vector('x')   # the data is presented as rasterized images\n",
    "    y1 = T.vector('y1')  # the labels are presented as 1D vector of\n",
    "    y2 = T.vector('y2')\n",
    "    z = T.vector('z')                    # [int] labels\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "\n",
    "    # Reshape matrix of rasterized images of shape (batch_size, 32 * 32)\n",
    "    # to a 4D tensor, compatible with our LeNetConvPoolLayer\n",
    "    # (32, 32) is the size of CIFAR-10 images.\n",
    "    layer0_input = x\n",
    "\n",
    "    # Construct the first convolutional pooling layer:\n",
    "    # filtering reduces the image size to (32-3+1 , 32-3+1) = (30, 30)\n",
    "    # maxpooling reduces this further to (30/2, 30/2) = (15, 15)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 15, 15)\n",
    "    # filter_shape: (number of filters, num input feature maps, filter height, filter width)\n",
    "    layer0 = RNNSLU3(\n",
    "        input=layer0_input,\n",
    "        nh = 150\n",
    "    )\n",
    "    w = theano.shared(name='w', value=0.2 * numpy.random.uniform(-1.0, 1.0, (150, 1026)).astype(theano.config.floatX))\n",
    "    b = theano.shared(name='b', value=numpy.zeros((1026,), dtype=theano.config.floatX))\n",
    "\n",
    "\n",
    "\n",
    "    tmp = T.nnet.relu(T.dot(layer0.output, w) + b)\n",
    "\n",
    "    tmp1 = tmp[:,:513]\n",
    "    tmp2 = tmp[:,513:]\n",
    "    #out1 = abs(tmp1)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    #out2 = abs(tmp2)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    #cost = T.mean((out1-y1.reshape((point,513)))**2+(out2-y2.reshape((point,513)))**2)\n",
    "    out1 = abs(tmp1)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "    out2 = abs(tmp2)/(abs(tmp1) + abs(tmp2)+10**(-8))*z.reshape((point,513))\n",
    "\n",
    "    if isd == True:\n",
    "        cost = IS(out1, y1.reshape((point,513))) + IS(out2,y2.reshape((point,513))) - IS(out2, y1.reshape((point,513)))/2 - IS(out1, y2.reshape((point,513)))/2\n",
    "\n",
    "    else:\n",
    "        cost = T.mean((out1-y1.reshape((point,513)))**2+(out2-y2.reshape((point,513)))**2)\n",
    "    #(sdr1, sir1, sar1,perm1) = mir_eval.separation.bss_eval_sources(out1.flatten(),z)\n",
    "    #(sdr2, sir2, sar2,perm2) = mir_eval.separation.bss_eval_sources(y1.flatten(),z)\n",
    "    #error =  sdr1-sdr2\n",
    "    error = cost\n",
    "    # create a function to compute the mistakes that are made by the model\n",
    "\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        error,\n",
    "        givens={\n",
    "            x: test_set_x[index],\n",
    "            y1: test_set_y1[index],\n",
    "            y2: test_set_y2[index],\n",
    "            z: test_set_z[index]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        error,\n",
    "        givens={\n",
    "            x: valid_set_x[index],\n",
    "            y1: valid_set_y1[index],\n",
    "            y2: valid_set_y2[index],\n",
    "            z: valid_set_z[index]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = [w, b] + layer0.params\n",
    "    '''\n",
    "    # create a list of gradients for all model parameters\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # train_model is a function that updates the model parameters by\n",
    "    # SGD Since this model has many parameters, it would be tedious to\n",
    "    # manually create an update rule for each model parameter. We thus\n",
    "    # create the updates list by automatically looping over all\n",
    "    # (params[i], grads[i]) pairs.\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "    '''\n",
    "    updates = Adam(cost, params, lr=0.0002, b1=0.1, b2=0.001, e=1e-8)\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates = updates,\n",
    "        givens={\n",
    "            x: train_set_x[index],\n",
    "            y1: train_set_y1[index],\n",
    "            y2: train_set_y2[index],\n",
    "            z: train_set_z[index]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # end-snippet-1\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training')\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 50  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        test_model(i)\n",
    "                        for i in range(n_test_batches)\n",
    "                    ]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f ') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    out_model = theano.function(\n",
    "        [index],\n",
    "        [out1,out2],\n",
    "        givens={\n",
    "            x: test_set_x[index],\n",
    "            z: test_set_z[index]\n",
    "        }\n",
    "    )\n",
    "    out_wav = out_model(0),\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f  obtained at iteration %i, '\n",
    "          'with test performance %f ' %\n",
    "          (best_validation_loss, best_iter + 1, test_score ))\n",
    "    print(('The code for file ' +\n",
    "           os.path.split(__file__)[1] +\n",
    "           ' ran for %.2fm' % ((end_time - start_time) / 60.)))\n",
    "    return out_wav\n",
    "\n",
    "def Adam(cost, params, lr=0.0002, b1=0.1, b2=0.001, e=1e-8):\n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    i = theano.shared(numpy.float32(0))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - (1. - b1)**i_t\n",
    "    fix2 = 1. - (1. - b2)**i_t\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    updates.append((i, i_t))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train with IS distance\n",
    "out2=test_DRNN(trdata ,tedata, vadata, learning_rate=0.1, n_epochs=1, batch_size=1, point = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_nsdr2=[]\n",
    "n_sir2=[]\n",
    "n_sar2=[]\n",
    "for i in range(len(out2)):\n",
    "    mag=out2[i][0]+out2[i][1]\n",
    "    y_hat=istft_mp(mag.T,outphase)\n",
    "    y_left=istft_mp(tedata[1][i].reshape(100,513).T,lphase)\n",
    "    y_right=istft_mp(tedata[2][i].reshape(100,513).T,rphase)\n",
    "    nsdr, sir, sar=evaluate(tedata[2][i],out2[i][1].T.flatten(),tedata[0][i])\n",
    "    n_nsdr2.append(nsdr)\n",
    "    n_sir2.append(sir)\n",
    "    n_sar2.append(sar)\n",
    "    prod_wav(istft_mp(out2[i][1].T,rphase),y_hat,y_right,i+1)\n",
    "gnsdr2, gsir2, gsar2=g_eval(n_nsdr2, n_sir2, n_sar2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnsdr2, gsir2, gsar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(5,1,1)\n",
    "plt.plot(tedata[0][0].reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('mixture')\n",
    "plt.subplot(5,1,2)\n",
    "plt.plot(out2[0][0].flatten().reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('recovered music')\n",
    "plt.subplot(5,1,3)\n",
    "plt.plot(tedata[1][0].reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('clean music')\n",
    "plt.subplot(5,1,4)\n",
    "plt.plot(out2[0][1].flatten().reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('recovered vocal')\n",
    "plt.subplot(5,1,5)\n",
    "plt.plot(tedata[2][0].reshape(513,100))\n",
    "plt.axis([0, 513, 0, 100])\n",
    "plt.title('clean vocal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
